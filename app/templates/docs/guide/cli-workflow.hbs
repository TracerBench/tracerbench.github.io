<article>
  <h2 id="intro">
    Command Line Interface Workflow
  </h2>
  <hr />
  <p>
    TracerBench is most commonly consumed two ways, via CLI and CI. The CLI allows for a developer to quickly run a
    performance benchmark locally. While leveraging TracerBench in CI, drastically aides in the flagging and prevention
    of a performance regression within a web application over time.</p>
  <p>The recommended way of leveraging TracerBench is via
    the <a target="_blank" href="https://www.npmjs.com/package/tracerbench">CLI</a>, which has two primary workflows:
  </p>

  <h2 id="compare">Compare Workflow</h2>
  <hr>
  <p> Comparing the performance delta between two builds, control vs experiment (most common workflow). This is
    executed with the <LinkTo @route='docs.api.compare'><code>`tracerbench compare`</code></LinkTo> command.
  </p>

  <h2 id="profile">Profile Workflow</h2>
  <hr>
  <p>Gaining insight into user-timings from a single Chrome trace. This is executed with the
    <LinkTo @route='docs.api.profile'><code>`tracerbench profile`</code></LinkTo> command.</p>

</article>
{{! 

# CLI

The recommended way of consuming TracerBench is via the [TracerBench-CLI](https://github.com/TracerBench/tracerbench/tree/master/packages/cli)

# CLI Workflow

Assuming the TracerBench-CLI is globally [installed](https://github.com/TracerBench/tracerbench/blob/master/packages/cli/README.md#usage) and you are leveraging the optional config file [tbconfig.json](https://github.com/TracerBench/tracerbench/blob/master/packages/cli/README.md#optional-config).

### Basic Example of benchmarking for timings

1. Start by having TracerBench record a HAR:

```console
$ tracerbench record-har --url http://localhost:8000 --cookiespath <path-to-cookies>

✔ DevTools listening on ws://<address>
✔ { timestamp: 241968.79908 }
✔ HAR recorded and available here: tracerbench.har
```

2. Now have TracerBench record a Trace of that HAR:

```console
$ tracerbench trace --url http://localhost:8000 --cookiespath <path-to-cookies> --harpath tracerbench.har --insights
...

✔ DevTools listening on ws://<address>
...
✔ starting trace
✔ navigating to http://localhost:8000
...
✔ stopping trace
✔ Trace JSON file successfully generated and available here: ./trace.json
```

3. Now that you have "trace.har" and "trace.json" files you can benchmark for the list of timings:

```console
$ tracerbench marker-timings --url http://localhost:8000
...

✔ Timings:       0.00 ms navigationStart
✔ Timings:       0.29 ms fetchStart
✔ Timings:      14.75 ms responseEnd
✔ Timings:      16.56 ms unloadEventStart
✔ Timings:      16.58 ms unloadEventEnd
✔ Timings:      16.91 ms domLoading
✔ Timings:      17.24 ms CommitLoad 14D19FA88C4BD379EA6C8D2BBEA9F939 http://localhost:8000/
✔ Timings:     362.58 ms domInteractive
✔ Timings:     362.64 ms domContentLoadedEventStart
✔ Timings:     363.22 ms domContentLoadedEventEnd
✔ Timings:     400.03 ms domComplete
✔ Timings:     400.06 ms loadEventStart
✔ Timings:     400.08 ms loadEventEnd
✔ Timings:     410.85 ms firstLayout
```

# TracerBench Compare: Manual Workflow

### Instrument your web application

In your app you must place a marker to let TracerBench know that you are done rendering to the DOM, it searches forward from this to find the next paint event. This is done by using a `performance.mark` function call. Additionally the [properties](https://raw.githubusercontent.com/TracerBench/tracerbench/master/docs/nav-timings.png) of the [NavigationTiming API](https://www.w3.org/TR/navigation-timing/#sec-navigation-timing-interface) can be passed a `markers` arguments array to the `InitialRenderBenchmark`.

![Understanding-NavTiming-API](https://github.com/TracerBench/tracerbench/blob/master/docs/nav-timings.png)

```js
function renderMyApp() {
  // basic "web application"
  // literally an app with a single empty p tag
  const p = document.createElement('p');
  document.body.appendChild(p);
}

function endTrace() {
  // just before paint
  requestAnimationFrame(() => {
    // after paint
    requestAnimationFrame(() => {
      document.location.href = 'about:blank';
    });
  });
}

// render the app
renderMyApp();

// marker renderEnd
performance.mark('renderEnd');

// end trace and transition to blank page
// internally cue tracerbench for another sample
endTrace();
```

In the example above we would mark right after we render the app and then call an `endTrace` function that ensures that we schedule after paint then transition to a blank page. Internally tracerbench will see this as the cue to start a new sample.

### Init Benchmark & Runner

The most common and recommended consumption of TracerBench is via the [TracerBench-CLI](https://github.com/TracerBench/tracerbench/tree/master/packages/cli). Optionally, TracerBench does however expose an API directly. The most basic consumption of this is via the `InitialRenderBenchmark` and `Runner` with the option to leverage [HAR-Remix](https://github.com/TracerBench/har-remix) to serve recorded HARs.

```js
import * as fs from 'fs-extra';
import { InitialRenderBenchmark, Runner } from '@tracerbench/core';

// the number of samples TracerBench will run. Higher sample count = more accurate.
// However the duration of the test will increase. The recommendation is somewhere between 30-60 samples.
const samplesCount = 40;

// the markers leveraged tuning your web application. additionally this assumes you have tuned
// your web application with the following marker "renderEnd"
// (see "Instrument your web application" above). the full list of available markers is robust,
// especially as it pertains to web application frameworks (ember, react etc).
// as a baseline the `PerformanceTiming` API is fully available
const markers = [{ start: 'domComplete', label: 'domComplete' }];
const browser = {
  type: 'canary',
  additionalArguments: [
    '--headless',
    '--disable-gpu',
    '--hide-scrollbars',
    '--mute-audio',
    '--v8-cache-options=none',
    '--disable-cache',
    '--disable-v8-idle-tasks',
    '--crash-dumps-dir=./tmp',
  ],
};

// name, url, markers and browser are all required options
const control = new InitialRenderBenchmark({
  // some name for your control app
  name: 'control',
  // serve your control tuned application locally or
  // via HAR Remix
  url: 'http://localhost:8001/',
  markers,
  browser,
  // location to save only the control trace to
  saveTraces: () => `./control-trace.json`,
});

const experiment = new InitialRenderBenchmark({
  name: 'experiment',
  url: 'http://localhost:8002/',
  markers,
  browser,
  // location to save only the experiment trace to
  saveTraces: () => `./experiment-trace.json`,
});

// the runner uses the config of each benchmark to test against
// the output of which
const runner = new Runner([control, experiment]);
runner
  .run(samplesCount)
  .then(results => {
    console.log(results);
    // optionally output the results using fs to a path of your choice
    // now its time for some statistical analysis (see "Statistical Analysis")
    fs.writeFileSync(`./trace-results.json`, JSON.stringify(results, null, 2));
  })
  .catch(err => {
    console.error(err);
    process.exit(1);
  });
```

### Trace-Results

The typings for "trace-results.json" are as follows:

- [samples: IITerationSample](https://github.com/TracerBench/tracerbench/blob/0508e9867b8bb8624739e16f0e812211a8346cc1/packages/tracerbench/src/benchmarks/initial-render-metric.ts#L73-L106)
- [phases: IPhaseSample](https://github.com/TracerBench/tracerbench/blob/0508e9867b8bb8624739e16f0e812211a8346cc1/packages/tracerbench/src/benchmarks/initial-render-metric.ts#L126-L141)
- [gc: IV8GCSample](https://github.com/TracerBench/tracerbench/blob/0508e9867b8bb8624739e16f0e812211a8346cc1/packages/tracerbench/src/benchmarks/initial-render-metric.ts#L39-L46)
- [blinkGC: IBlinkGCSample](https://github.com/TracerBench/tracerbench/blob/0508e9867b8bb8624739e16f0e812211a8346cc1/packages/tracerbench/src/benchmarks/initial-render-metric.ts#L48-L51)
- [runtimeCallStats?: IRuntimeCallStats](https://github.com/TracerBench/tracerbench/blob/0508e9867b8bb8624739e16f0e812211a8346cc1/packages/tracerbench/src/benchmarks/initial-render-metric.ts#L53-L71)

```ts
[{
  "meta": {
    "browserVersion": string,
    "cpus": string[]
  },
  "samples": IITerationSample[{
    "duration": number,
    "js": number,
    "phases": IPhaseSample[],
    "gc": IV8GCSample[],
    "blinkGC": IBlinkGCSample[],
    "runtimeCallStats": IRuntimeCallStats
  }],
  "set": string
}]
```

### Statistical Analysis

Assuming you have the output results ("trace-results.json") from your TracerBench run, its time to perform statistical analysis on the [Trace-Results](#Trace-Results) JSON file.

When running the TracerBench-CLI `compare` command, on a successful trace a stdout statistical summary report will be generated. Additionally an included `compare --report` flag will create a PDF and HTML report.

TracerBench also exposes an explicit `tracerbench report` command that takes a path to the folder containing your "trace-results.json" file and will create a PDF and HTML report.

### Understanding The Box-Plot Results & Stats

TracerBench Statistics Primer available within the exported `Stats` class: https://github.com/TracerBench/tracerbench/tree/master/packages/stats

![box-plot-results](https://github.com/TracerBench/tracerbench/blob/master/docs/box-plot-transparent.png) }}